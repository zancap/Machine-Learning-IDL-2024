{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69d4d223-581b-417a-b6ff-f0950b2ec9d9",
   "metadata": {},
   "source": [
    "# TP2 : Perceptron multicouches pour l'analyse de sentiment\n",
    "\n",
    "Le but de ce TP va être d'entraîner et de comparer plusieurs réseaux de neurones de type et de taille différent sur une tâche textuelle, à savoir l'analyse de sentiment.\n",
    "Maintenant que nous savons comment fonctionne un réseau de neurones, nous allons pouvoir utiliser des bibliothèques Python qui vont grandement facilitent grandement la création et l'entraînement de réseaux.\n",
    "\n",
    "Ce TP ne demande pas d'écriture de code, n'hésitez donc pas à modifier les divers paramètres pour observer ce qui se passe !\n",
    "\n",
    "**Date limite de rendu :** Le 11 octobre 2024 à 10:30.\n",
    "\n",
    "## Préparatifs\n",
    "\n",
    "Les commandes suivantes permettent d'installer et de charger les bibliothèques Python nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d5763-a35d-4395-9e66-0f0908174827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de keras et de tensorflow (tensorflow étant une grosse bibliothèque, cela peut prendre un certain temps)\n",
    "# Cette cellule n'a besoin d'être exécutée qu'une seule fois en tout et pour tout\n",
    "!pip install keras Keras-Preprocessing tensorflow nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be228e6-584f-4f95-a073-aee391e80466",
   "metadata": {},
   "source": [
    "**Attention :** Après avoir exécuté la cellule ci-dessus, il faut impérativement redémarrer le noyau Python pour que Python puisse reconnaître le module nouvellement installé.\n",
    "\n",
    "La manière de faire cela dépend de votre éditeur :\n",
    "\n",
    "- Sur Jupyter : `Noyau -> Redémarrer` (ou `Kernel -> Restart` si l'instance est en anglais)\n",
    "- Sur Colab : `Runtime -> Restart session` : <div><img src=\"img/colab.png\" width=\"500\"/></div>\n",
    "- Sur VSCode : Un bouton `Restart`/`Redémarrer` : <div><img src=\"img/vscode.png\" width=\"500\"/></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e7502-a884-4215-95a7-5795e424b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques:\n",
    "# - random pour la génération de nombres aléatoires et la reproducibilité\n",
    "# - numpy pour la gestion des matrices\n",
    "# - tensorflow pour la gestion de réseaux de neuroens\n",
    "# - keras pour la création et l'entraînement simplifié des réseaux (keras est une interface haut niveau qui utilise tensorflow)\n",
    "# - pandas pour le chargement et la visualisation des données sous forme de tableau\n",
    "# - scikit-learn, nltk et re pour le pré-traitement des données\n",
    "# - matplotlib pour les graphiques\n",
    "\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Reproducibilité\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268ec7d-cb31-4ee8-a029-868c99e8dfe8",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7ffee-f8ef-43c4-a295-1ee580e85faf",
   "metadata": {},
   "source": [
    "Le jeu de données sur lequel nous allons travailler est issu de [Kaggle](https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset). Il s'agit d'un ensemble de tweets qui ont été automatiquement annoté pour de l'analyse de sentiment. Autrement dit, pour un tweet donné, on cherche à savoir si son contenu est positif, négatif ou neutre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbabe1-f6b4-4b13-bdae-d71029287e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "df = pd.read_csv('data.csv',encoding='latin1')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b5452c-96eb-4749-bd83-274e0a732420",
   "metadata": {},
   "source": [
    "Le jeu de données contient, en plus du tweet et de son étiquette (positif/négatif/neutre), beaucoup d'informations supplémentaires comme l'âge et le pays de l'utilisateur, ainsi que l'heure de la journée à laquelle le tweet a été écrit. Dans ce TP, nous n'utiliserons que le texte.\n",
    "\n",
    "Notre but va être de prédire l'étiquette d'un tweet à partir de son texte. Il s'agit donc à nouveau d'un problème de **classification**.\n",
    "\n",
    "Comme la dernière fois, la première étape consiste à transformer les étiquettes en probabilités. Nous avons à nouveau 3 types d'étiquettes possible, donc 3 probabilités pour chaque point de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ce55a-0bdf-4888-b655-dac5d9eb996a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On garde uniquement les deux colonnes qui nous intéressent, et on se débarrasse des lignes contenant des champs vides (N/A)\n",
    "df = df[['text', 'sentiment']].dropna()\n",
    "\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "labels = enc.fit_transform(df['sentiment'].values.reshape(-1, 1))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7009f8-937c-4838-bdf6-fc4c56739113",
   "metadata": {},
   "source": [
    "Il nous faut désormais transformer le texte de chaque tweet en données numériques. Beaucoup d'approches sont possibles pour cela, dont en voici certaines :\n",
    "- L'encodage 1 parmi n (vu précédemment), qui transforme chaque mot en une liste de zéros contenant un seul 1 à une position correspondant au mot\n",
    "- La TF-IDF, qui transforme un texte en une liste de fréquences relatives de mots\n",
    "- Les plongements sémantiques (*embedding*), qui transforme chaque mot du texte initial en une liste de nombres calibrée de manière à représenter le sens du mot\n",
    "\n",
    "Nous allons dans un premier temps nous focaliser sur la TF-IDF.\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "La [TF-IDF](https://fr.wikipedia.org/wiki/TF-IDF) (term frequency/inverse document frequency) est une métrique qui associe à un document (texte) et à un mot donné un score d'importance compris entre 0 et 1. Nous ne nous attarderons pas sur sa formulation mathématique au cours de ce TP, mais elle utilise les principes suivants :\n",
    "- Si un mot est fréquent dans un document mais dans peu d'autres documents, alors ce mot est spécifique à ce document et a un score élevé\n",
    "- Si un mot est fréquent dans un grand nombre de documents, alors ce mot est commun et son score est plus faible.\n",
    "\n",
    "La librairie `scikit-learn` contient un outil permettant de générer automatiquement la TF-IDF d'un ensemble de documents; elle s'occupe de diviser chaque document en mot, puis calcule le score de chaque mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b37ec-bde9-46fd-958c-82839fb1baa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorization = TfidfVectorizer()\n",
    "\n",
    "# Calcule la TF-IDF de chaque mot dans chaque texte\n",
    "tfidf = vectorization.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a76789-3459-46e1-975b-72abaa340515",
   "metadata": {},
   "source": [
    "La matrice `tfidf` contient désormais en position (i, j) la TF-IDF du mot numéro `j` dans le document numéro `i`.\n",
    "Les numéros associés à chaque mot peuvent être lus en accèdant au dictionnaire `vectorization.vocabulary_`.\n",
    "\n",
    "**À noter :** Par défault, `TfidfVectorizer` transforme tout le texte en minuscule et ignore une partie de la ponctuation.\n",
    "\n",
    "Par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44713ab7-653e-4eb3-a778-3045eeda9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_doc = 1\n",
    "print('Document :', df['text'][num_doc])\n",
    "\n",
    "num_mot = vectorization.vocabulary_['in']\n",
    "print('TF-IDF pour \"in\" :   ', tfidf[num_doc, num_mot])\n",
    "\n",
    "num_mot = vectorization.vocabulary_['diego']\n",
    "print('TF-IDF pour \"diego\" :', tfidf[num_doc, num_mot])\n",
    "\n",
    "num_mot = vectorization.vocabulary_['dog']\n",
    "print('TF-IDF pour \"dog\" :  ', tfidf[num_doc, num_mot])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3a801-7ba9-4771-ac12-489c8ebb7816",
   "metadata": {},
   "source": [
    "### Séparation et mélange\n",
    "\n",
    "On rappelle qu'en apprentissage machine, il est important de séparer ses données en trois jeux :\n",
    "- Le jeu d'entraînement, servant à entraîner les modèles,\n",
    "- Le jeu de validation, qui sert à mesurer les performances des divers modèles que nous allons entraîner et à les comparer\n",
    "- Le jeu de test, qui sert à mesurer les performances finales (vraies) du modèle retenu.\n",
    "\n",
    "Nous pouvons réaliser la séparation grâce à la fonction `train_test_split`, comme au TP 1.\n",
    "\n",
    "En général, on choisit de garder la majorité des données pour l'entraînement, et autant de données pour la validation que pour le test. Les ratios les plus couramment utilisés pour les trois jeux sont 60%-20%-20%, 80%-10%-10% et 90%-5%-5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb41d21-2a85-41e0-a192-add1982c886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "valid_frac = 0.1\n",
    "test_frac = 0.1\n",
    "\n",
    "# On sépare d'abord le jeu d'entraînement du reste\n",
    "X_train, X_2, y_train, y_2 = train_test_split(tfidf, labels, test_size=valid_frac + test_frac, shuffle=True, random_state=seed)\n",
    "# Puis on sépare le reste en deux (validation et test)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_2, y_2, test_size=test_frac/(test_frac + valid_frac), shuffle=True, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e2d8c-42e2-4912-9cc5-c35fe12ee441",
   "metadata": {},
   "source": [
    "**Q1 :** L'ensemble d'entraînement contient 21 984 documents, et nous avons 3 étiquettes par exemple. À quoi correspond le nombre 26 397 ?\n",
    "\n",
    "**Réponse :** 26 397 correspond à la largeur de la matrice TF-IDF, autrement dit le nombre de mots uniques trouvés dans l'intégralité du corpus. Chaque document obtient 26 397 scores (un pour chaque mot). Si un mot n'est pas présent dans un document, le score de ce mot pour ce document est automatiquement 0. La matrice contient donc en majorité des 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14d2ea-1a62-4b23-a803-6b41b1834326",
   "metadata": {},
   "source": [
    "# Réseau de neurones\n",
    "\n",
    "Comme au premier TP, nous allons commencer par créer un réseau de neurones à une couche (entrée -> sortie).\n",
    "\n",
    "En plus de cela, notre réseau de neurones utilisera les éléments suivants :\n",
    "- Fonction d'activation : Softmax (utilisée dans le cas d'une classification à plus de deux classes, alors que la sigmoïde est plutôt utilisée dans le cas à deux classes)\n",
    "- Fonction de coût : Entropie croisée (utilisée en classification)\n",
    "\n",
    "Enfin, pour effectuer la descente de gradient, diverses méthodes existent. La méthode que nous avons utilisée la dernière fois est la plus simple, mais elle n'est pas très efficace. Beaucoup d'autres méthodes plus complexes existent, faisant par exemple varier le taux d'apprentissage au cours du temps (*momentum*) ou en gardant en mémoire un taux d'apprentissage différent pour chaque poids du réseau, par exemple. La composante qui s'occupe de gérer cette partie s'appelle un *optimiseur*. Nous utiliserons ici l'optimiseur [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam).\n",
    "\n",
    "Par chance, Keras rend l'implémentation de tout ceci extrêmement simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e802c-b7f7-448e-aac7-bac5205ec156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibilité\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "# Création d'un réseau de neurones (liste de couches)\n",
    "model = Sequential()\n",
    "\n",
    "# Ajout d'une couche simple (dense), avec 3 sorties, et utilisant le Softmax comme fonction d'application\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Finalise la création du modèle en utilisant l'entropie croisée pour fonction de coût, l'optimiseur Adam et en mesurant la précision du modèle à chaque époque\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîne le modèle pendant 30 itérations sur le jeu d'entraînement, et en mesurant la précision sur les jeux d'entraînement et de validation\n",
    "# Cet étape prend une minute environ\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Affiche les couches du modèles et leur nombre de poids (paramètres)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f7e7f6-4c27-4b54-9011-c341d5cc6cd6",
   "metadata": {},
   "source": [
    "Pratique, non ?\n",
    "\n",
    "On peut également facilement visualiser l'évolution de la précision au cours de l'entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa7eb0-fa25-4e0b-9603-ba8f2941b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_accuracy(history):\n",
    "    plt.plot(history.history['accuracy'], label='Précision (entraînement)')\n",
    "    plt.plot(history.history['val_accuracy'], label='Précision (validation)')\n",
    "    plt.xlabel('Itération')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "display_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a93bc20-d2c8-45cc-9991-fb46a0005258",
   "metadata": {},
   "source": [
    "**Q2 :** D'après ce graphique, à quelle itération arrêteriez-vous l'entraînement et pourquoi ?\n",
    "\n",
    "**Réponse :** Il est ici possible de s'arrêter à la 15e itération. En effet, la précision sur l'ensemble de validation (qui témoigne des performances réelles du modèle) n'augmente plus. La précision d'entraînement continue d'augmenter, mais cela témoigne uniquement d'un surapprentissage possible.\n",
    "\n",
    "# Utilisation du réseau\n",
    "\n",
    "Faisons un test sur un exemple simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a96fac-803a-474b-8937-31145bb6a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_text = \"The movie was so bad, I will not recommend this movie to anyone\"\n",
    "\n",
    "ex_tfidf = vectorization.transform([ex_text])\n",
    "out = model(ex_tfidf.toarray())\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184d79b-1285-4eab-8f03-1259ebd01bf4",
   "metadata": {},
   "source": [
    "Comment savoir quelle probabilité correspond à telle étiquette ? Nous pouvons regarder le contenu de l'encodeur 1 parmi n pour répondre à cette question :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45bc2f-51bc-434b-8c84-9dc7b6a45abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc.inverse_transform([[1, 0, 0]]))\n",
    "print(enc.inverse_transform([[0, 1, 0]]))\n",
    "print(enc.inverse_transform([[0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f2685-8c1a-4c1f-83d8-3ff552be3da1",
   "metadata": {},
   "source": [
    "La première probabilité correspond donc à l'étiquette négative, la deuxième à la neutre et la dernière à la positive. Créons une fonction pour automatiser tout cela :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8811d7-18a4-4835-b8b2-68e473815cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(pos):\n",
    "    label = [0, 0, 0]\n",
    "    label[pos] = 1\n",
    "    return enc.inverse_transform([label])[0][0]\n",
    "\n",
    "def predict(ex_text, model, display=True):\n",
    "    ex_tfidf = vectorization.transform([ex_text])\n",
    "    out = model(ex_tfidf.toarray())\n",
    "    out = out.numpy()\n",
    "    sentiment = get_label(out.argmax())\n",
    "    if display:\n",
    "        return ex_text + ' -> ' + sentiment\n",
    "    return sentiment\n",
    "\n",
    "print(predict(\"I feel sad today\", model))\n",
    "print(predict(\"I love my mom\", model))\n",
    "print(predict(\"Today is Tuesday\", model))\n",
    "print(predict(\"Good riddance!\", model))  # <- Erreur de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3049cd98-7996-4a5a-af73-fe6df27dcd72",
   "metadata": {},
   "source": [
    "Comme ce réseau ne contient qu'une seule couche, on peut considérer qu'il attribue en fait un poids positif ou négatif à chaque mot du vocabulaire pour chaque étiquette possible, et qu'il calcule ensuite la somme des poids des mots contenus dans un texte, pondérés par la TF-IDF du mot donné.\n",
    "\n",
    "On peut d'ailleurs observer ces poids directement dans le modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b248b272-a721-4e33-94a5-468376a0b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mot = \"angry\"\n",
    "num_mot = vectorization.transform([mot]).argmax()\n",
    "weights = model.layers[0].get_weights()[0][num_mot]\n",
    "print(f\"Poids pour le mot [{mot}]:\")\n",
    "for i, weight in enumerate(weights):\n",
    "    print(f\"{get_label(i):<10} {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7642ac1-e629-4d2b-9292-b6b81e73f775",
   "metadata": {},
   "source": [
    "# 2 couches et plus\n",
    "\n",
    "Globalement, le modèle obtenu fonctionne plutôt bien. Cependant, le score de précision nous indique que près d'un tiers de ses prédictions sont erronnées. Nous aimerions l'améliorer, en lui permettant de faire des calculs plus complexes qu'une simple somme de poids. Pour cela, nous allons transformer notre perceptron en **perceptron multicouche** (MLP, *multilayer perceptron*). Il s'agit tout simplement de plusieurs perceptrons simples mis bout à bout :\n",
    "\n",
    "<div><img src=\"img/mlp.png\" width=\"500\"/></div>\n",
    "\n",
    "L'intérêt d'avoir plusieurs couches est que le réseau peut faire des opérations plus complexes : Si un simple perceptron à une couche lui permettait de faire une somme pondérée des scores des mots, ajouter une autre couche lui permet par exemple de calculer des sous-scores correspondant à certaines catégories de mots. D'ailleurs, un [théorème]([https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_d%27approximation_universelle) dit qu'un réseau de neurones possédant deux couches suffisamment grandes (une couche cachée et une couche de sortie) est en théorie capable d'apprendre n'importe quelle fonction ! En pratique, les réseaux ont souvent plus de deux couches, mais qui sont moins grandes.\n",
    "\n",
    "Savoir quelle fonction d'activation, combien de couches et quelle taille de couche utiliser est difficile; en pratique, même si quelques guides généraux existent, il s'agit surtout de tester différentes valeurs et de regarder ce qui marche le mieux...\n",
    "\n",
    "On notera toutefois que pour les couches cachées, un principe général est d'utiliser une taille qui soit une puissance de 2 (2, 4, 8, 16, 32 etc. neurones).\n",
    "\n",
    "Rajouter des couches se fait très simplement avec Keras, il suffit de définir leur fonction d'activation. Pour les couches intermédiaires d'un réseau de neurones, plusieurs fonctions d'activation existent, donc les plus courantes sont `relu`, `sigmoid` et `tanh`.\n",
    "\n",
    "Voici donc un exemple à deux couches :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221ff82-f219-4a06-9a5f-d14e02096805",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(seed)\n",
    "model2 = Sequential()\n",
    "\n",
    "# Ajout d'une couche cachée avec 16 sorties, utilisant la fonction d'activation sigmoïde\n",
    "model2.add(Dense(16, activation='sigmoid'))\n",
    "# Ajout de la couche de sortie, avec 3 sorties, et utilisant le Softmax comme fonction d'application\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Entraînement et affichage\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model2.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "model2.summary()\n",
    "display_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100c007-0789-4afb-825c-c01159c69643",
   "metadata": {},
   "source": [
    "**Q3 :** Faites quelques essais supplémentaires avec différentes tailles de couches, ainsi qu'en modifiant le nombre d'itérations. Vous pouvez également rajouter une couche supplémentaire similaire à la première, avec la taille de sortie de votre choix. Sans passer plus de 5-10 minutes sur cette question, arrivez-vous à obtenir une précision de validation qui soit supérieure à 70\\% ?\n",
    "\n",
    "Donnez un exemple de paramètres que vous avez testés et les résultats obtenus (meilleure précision de validation atteinte durant l'entraînement), puis calculez la précision atteinte sur le jeu de test grâce au code ci-dessous :\n",
    "\n",
    "**Réponse :** Dépend de ce que vous avez. Il est possible mais très difficile de dépasser les 70% de précision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfb29d-5601-4da5-8f2b-90e5e68d14bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la précision sur le jeu de test\n",
    "y_pred = model2(X_test.toarray()).numpy().argmax(axis=1)\n",
    "y_true = y_test.argmax(axis=1)\n",
    "print(f\"Précision (test): {(y_pred == y_true).sum() / len(y_pred):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21f264-b1d2-416a-8053-9fdd1f66e023",
   "metadata": {},
   "source": [
    "**Attention :** On rappelle que bien que la précision globale soit une métrique utile, elle peut cacher des disparités entre les différentes classes. Selon les applications, il peut être préférable d'avoir un modèle qui soit globalement moins performant, mais qui soit aussi performant sur toutes les classes.\n",
    "\n",
    "Observons donc la répartition des étiquettes dans le jeu de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d362c21-77be-49fe-8784-4bae47d654a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2141f-fc46-4169-905e-3bc3e8280ce8",
   "metadata": {},
   "source": [
    "Ici, les trois classes sont représentées avec des proportions assez égales dans le jeu de données. Cependant, il peut être utile d'avoir d'observer à quel point celle-ci peut varier au sein des classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c02ba-5bde-45c6-a0af-9fb0d71a90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[get_label(x) for x in range(3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dab36-cbd3-4b59-a60d-70789307e38c",
   "metadata": {},
   "source": [
    "**Q3 bis :** Comment varie la précision de votre modèle au sein des trois classes ?\n",
    "\n",
    "**Réponse :** dépend de ce que vous avez. En général, la précision sur les exemples positifs et négatifs est assez bonne, et moins sur les exemples neutres; en effet, il est difficile de dire exactement ce qui rend un tweet neutre, alors que certains mots sont très liés à un sentiment positif ou négatif.\n",
    "\n",
    "### Taille du réseau\n",
    "\n",
    "À la fin de l'entraînement, Keras affiche le nombre de paramètres que contient votre modèle, c'est-à-dire son nombre de poids et biais. Le modèle de la semaine dernière contenait un total de 12 poids (matrice de taille (3, 4)) et 3 biais, soit 15 paramètres. Ici, notre modèle à une couche contient 79 194 paramètres à entraîner (*trainable params*), il est donc beaucoup plus gros ! C'est souvent le cas pour les modèles travaillant sur du texte, qui ont besoin de beaucoup de poids pour être capable de modéliser un vocabulaire entier.\n",
    "\n",
    "De plus, le nombre de paramètres d'un modèle augmente avec sa profondeur (nombre de couches). Les modèles de langue tels que celui utilisé par ChatGPT contiennent plusieurs milliards, voire dizaines de milliards de paramètres. Plus un modèle a de paramètres, plus l'entraîner ou l'utiliser prend de la mémoire et du temps.\n",
    "\n",
    "**Q3 ter :**  Combien de paramètres contient votre réseau de neurones ?\n",
    "\n",
    "**Réponse :** dépend de ce ce que vous avez. Il faut bien lire la ligne *trainable params* (paramètres du réseau de neurones) et non pas la ligne *total params*, qui inclut les paramètres de l'optimiseur ne servant que pendant l'entraînement mais supprimés lors de l'inférence.\n",
    "\n",
    "## Analyse des résultats\n",
    "\n",
    "Le modèle semble ne pas avoir d'amélioration significative, malgré l'ajout d'une couche supplémentaire. Observons quelques-unes des erreurs faites par le modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5ff4b-336a-40af-8210-fa4efbcbed58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_errors = 10\n",
    "errors = 0\n",
    "for idx, row in df.iterrows():\n",
    "    pred = predict(row['text'], model, False)\n",
    "    true = row['sentiment']\n",
    "    if pred != true:\n",
    "        print(f'Prédiction : [{pred}] au lieu de [{true}] pour le tweet [{row[\"text\"]}]')\n",
    "        errors += 1\n",
    "        if errors >= max_errors:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d01eaf-00b4-4dad-ba17-83ea1b6cdc3f",
   "metadata": {},
   "source": [
    "**Q4 :** D'après vous, d'où peuvent venir les erreurs de classification faites par le modèle ? N'hésitez pas à traduire le contenu des tweets ci-dessus en français si besoin.\n",
    "\n",
    "**Réponse :** On peut observer plusieurs phénomènes, dont entre autres :\n",
    "- Mots détectés comme positifs utilisés dans un contexte négatif ou sarcastique (\"fun\")\n",
    "- Composition de mots neutres donnant un sentiment positif ou négatif (\"give in\")\n",
    "- Mots rares/argotiques rares dans le corpus d'entraînement\n",
    "- Difficulté de savoir ce qu'est un tweet \"neutre\"\n",
    "\n",
    "Il est possible de visualiser les résultats du modèle grâce à une **matrice de confusion**. Il s'agit d'un tableau contenant en abscisse les étiquettes prédites par le modèle, et en ordonnée les étiquettes véritables. Chaque case contient alors le nombre d'exemples correspondant à une prédiction donnée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f89bb-0d17-4864-b09f-30b9859bcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X_test.toarray()).numpy().argmax(axis=1)\n",
    "y_true = y_test.argmax(axis=1)\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=[get_label(x) for x in range(3)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c642330-519a-4fb5-8b73-225b061658b4",
   "metadata": {},
   "source": [
    "Ici par exemple, la première rangée indique que 449 + 305 + 32 = 786 tweets étaient étiquetés comme négatifs dans le jeu de données. Sur ces 449, le modèle en a correctement étiqueté 449 comme négatifs, mais en a également étiqueté 305 comme neutres et 32 comme négatifs.\n",
    "\n",
    "Similairement, la première colonne indique que sur 449 + 178 + 32 tweets que le modèle a décrit comme négatifs, seuls 449 étaient vraiment négatifs, 178 étaient en fait neutres, et 32 étaient positifs.\n",
    "\n",
    "La jauge de couleur permet d'identifier facilement où sont les erreurs; dans l'idéal, la diagonale de la matrice de confusion doit être jaune, et le reste le plus violet possible. Ici, on remarque que le modèle a du mal à distinguer entre les tweets neutres et positifs/négatifs, mais qu'il étiquette très rarement un tweet négatif comme positif ou un positif comme négatif. D'autre part, la distinction entre un tweet négatif (ou positif) et un tweet neutre est assez floue et subjective. On peut donc imaginer que s'il n'y avait que ces deux types d'étiquettes, le modèle obtiendrait de meilleurs résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d1f54-4874-4d5d-a295-76087c89dd4a",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "Si vous avez terminé le TP avant la fin du temps imparti, voici deux tâches supplémentaires, que vous pouvez faire dans l'ordre de votre choix. Si vous les faites, gardez-en une trace dans votre notebook :\n",
    "\n",
    "## Partie 1 : Critiques de films (en français)\n",
    "\n",
    "Le fichier `data_b.csv` contient un autre jeu de données, cette fois-ci composé de critiques de films enregistrées sur AlloCiné (en français). Au texte de chaque critique est associé un score de sentiment qui vaut soit 0 (critique négative), soit 1 (critique positive).\n",
    "\n",
    "Dans une nouvelle cellule (ou plusieurs), copiez le code de la partie principale de ce TP et adaptez-le afin d'entraîner un nouveau réseau de neurones sur les critiques de films. Très peu de choses sont à changer, la différence principale étant qu'il y a ici seulement deux étiquettes (positif/négatif) et non pas trois. En séparant toujours bien les données en jeux d'entraînement, de validation et de test, essayez ensuite d'entraîner un réseau de neurones afin d'obtenir les meilleures performances possibles. Il est facile d'obtenir une précision atteignant 90 à 91\\%, pouvez-vous faire plus ? N'hésitez pas à expérimenter en changeant les couches, leur taille, leurs fonctions d'activation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f805941-6729-473b-b991-074c6e97d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du fichier contenant les critiques de films... À vous d'écrire le reste !\n",
    "df = pd.read_csv('data_b.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6a68a-88d6-4ec5-88a0-7805a9563fa9",
   "metadata": {},
   "source": [
    "## Partie 2 : Pré-traitement de texte avancé\n",
    "\n",
    "La classe `TfidfVectorizer` a [beaucoup de paramètres optionnels](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). En particulier, elle permet de pré-traiter et nettoyer le texte avant d'obtenir sa TF-IDF, avec par exemple :\n",
    "- La suppression des accents et caractères spéciaux (paramètre `strip_accents`)\n",
    "- Le passage du texte en minuscules (paramètre `lowercase`)\n",
    "- La suppression des mots courants tels que \"the\", \"and\", etc. (paramètre `stop_words`)\n",
    "- L'enregistrement d'expression multimots : plutôt que de calculer la TF-IDF pour les mots individuels, on peut également la calculer pour des suites (n-grams) de 2 ou plus mots consécutifs (on peut par exemple imaginer que le mot \"very\" soit neutre, mais que les expressions \"very good\" ou \"very bad\" soient de plus forts indicateurs que seulement \"good\" et \"bad\"). Ceci peut être contrôlé grâce au paramètre `ngram_range`. Attention, si vous modifiez ce paramètre, il est fortement conseillé d'également utiliser le paramètre `max_features` pour limiter la taille du dictionnaire à (par exemple) 30 000 mots ou suites de mots, ou vous risquez de manquer de mémoire.\n",
    "- Le fait d'ignorer des mots trop rares ou trop fréquents (paramètres `min_df` et `max_df`)\n",
    "\n",
    "Tentez d'expérimenter avec ces paramètres afin de voir si vous arrivez à améliorer les performances du réseau de neurones (soit pour les tweets, soit pour les critiques de films)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ba42b-87e2-4a6a-a5a9-45169e5959af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
