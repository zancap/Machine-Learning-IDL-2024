{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2cf8170-5265-4a5e-999c-8cdd6fd769ee",
   "metadata": {
    "id": "e2cf8170-5265-4a5e-999c-8cdd6fd769ee"
   },
   "source": [
    "# TP3 : Modèles de langage appliqués à la traduction automatique\n",
    "\n",
    "<i>TP adapté de ceux de Yongxin Zhou, eux-même inspiré du TP de ALPS 2021, écrit par Alexandre Bérard</i>\n",
    "\n",
    "**Date limite de rendu :** Le 13 octobre 2024 à 10:30.\n",
    "\n",
    "Ce TP ne demande pas d'écriture de code. Il est en revanche plus long et contient beaucoup plus de questions que le précédent. Pour y répondre, inutile d'écrire beaucoup de texte : une ou deux phrases suffisent en général à chaque question.\n",
    "\n",
    "De plus, il n'est pas nécessaire que vous lisiez en détail chaque ligne du code qui est donné, mais il faut que vous compreniez ce qu'il se passe dans les grandes lignes. Les questions posées sont justement prévues pour cela.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "### But du TP\n",
    "Dans ce TP, nous allons entraîner et utiliser des modèles de langage pour la traduction automatique. Contrairement aux précédents TP, il ne s'agira plus d'une tâche de classification simple (données d'entrée -> classe) ; cette fois, nous disposons d'une *séquence* d'entrée (la phrase source) que nous souhaitons transformer en une séquende de sortie (la phrase traduite). Le type d'architecture neuronale que nous allons utiliser pour cela s'appelle donc en toute logique *seq2seq* (séquence vers séquence).\n",
    "\n",
    "## Préparatifs\n",
    "\n",
    "**Important :** Ce TP requiert d'avoir un GPU afin d'accélerer l'entraînement, sinon celui-ci dure beaucoup trop longtemps. Il est donc fortement conseillé d'exécuter ce notebook sur Google Colab.\n",
    "Avant de commencer à exécuter les cellules suivantes ou d'importer les fichiers, il est également nécessaire de changer les paramètres de Colab afin de s'assurer d'avoir accès à un GPU. Cela peut se faire à l'aide du menu `Session > Changer le type de session` et en sélectionnant `GPU T4`.\n",
    "\n",
    "Comme au TP précédent, vous aurez besoin d'importer les fichiers auxilliaires (`nmt_dataset.py`, `nnet_models.py`, `prepare.py`) ainsi que les images dans l'instance Colab.\n",
    "\n",
    "Les commandes suivantes permettent d'installer et de charger les bibliothèques Python nécessaires. Comme au cours du TP précédent, n'oubliez pas de redémarrer le noyau Python après installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce9d6c-9d6d-42c0-a125-5fb7ff0ed5f2",
   "metadata": {
    "id": "e6ce9d6c-9d6d-42c0-a125-5fb7ff0ed5f2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Installation des librairies\n",
    "!pip install torch jupyter subword-nmt sacremoses googletrans==3.1.0a0 pandas sacrebleu matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248cadaf-7dad-4672-8c10-95a46058fd91",
   "metadata": {
    "id": "248cadaf-7dad-4672-8c10-95a46058fd91"
   },
   "outputs": [],
   "source": [
    "# Chargement des bibliothèques\n",
    "\n",
    "# Fonctions auxilliaires prédéfinies\n",
    "import nmt_dataset\n",
    "import nnet_models\n",
    "\n",
    "# PyTorch, une bibliothèque d'apprentissage machine similaire à Tensorflow (que Keras utilise)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Bibliothèques Python standard\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "# Chargement et traitement des données et des matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation sous forme de graphiques\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Barre de chargement pour les opérations lentes\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Outil aidant à la construction du tokenizer\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "\n",
    "# Instance de Google Traduction, qui nous servira de référence\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d82a13-8db8-4d67-9520-866a15260f29",
   "metadata": {
    "id": "98d82a13-8db8-4d67-9520-866a15260f29"
   },
   "source": [
    "# Préparation du corpus de traduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7400194-bc09-4af5-9c47-7fc56df454ce",
   "metadata": {
    "id": "e7400194-bc09-4af5-9c47-7fc56df454ce"
   },
   "source": [
    "Le modèle que nous allons développer dans ce TP est un modèle de traduction de l'anglais vers le français. Afin de l'élaborer, nous utiliserons les données du projet [Tatoeba](https://tatoeba.org/fr), qui sont distribuées sous licence libre Creative Commons et hebergées par [Anki](www.manythings.org/anki).  \n",
    "\n",
    "Pour pouvoir entraîner un modèle de traduction, il nous faut disposer d'un corpus *parallèle*, c'est-à-dire constitué d'une collection de phrases sources (`source_lang`) et de traductions de référence (`target_lang`). Par parallèle, on entend que ces deux collections doivent être alignées phrase par phrase. Dans notre cas, elles seront stockées dans deux fichiers, de telle sorte que chaque ligne du fichier de référence soit la traduction de la même ligne dans le fichier source.\n",
    "\n",
    "Vous avez vu en cours que les corpus textuels doivent en général être pré-traités, ce que nous avons ignoré pour des raisons de simplicité au cours du TP précédent. Une des étapes les plus importantes du pré-traitement est la tokenization, qui consiste à découper chaque document (phrase) en plus petit morceaux appelés *tokens*. L'outil effectuant cette découpe s'appelle un *tokenizer*, et il en existe divers types, qui découpent par exemple le texte initial en mots, en sous-mots ou en caractères individuels. Ici, nous utiliserons un tokenizer en sous-mots de type *Byte Pair Encoding* (BPE), en utilisant la bibliothèque [`subword_nmt`](https://github.com/rsennrich/subword-nmt). Cette bibliothèque a la particularité qu'elle peut s'utiliser même hors de Python, directement dans le terminal. C'est ce que nous allons faire.\n",
    "\n",
    "Comme le reste du modèle, le tokenizer doit être entraîné afin de déterminer quelles découpes ont du sens, en fonction des sous-mots qui sont les plus fréquents. Ici, le modèle que nous entraînons est un modèle BPE français-anglais, capable de faire la tokenization de phrases anglaises et françaises.\n",
    "\n",
    "### Apprentissage du tokenizer\n",
    "La cellule effectue les opérations suivantes :\n",
    "- Passage en mode shell (exécution des commandes dans le terminal)\n",
    "- Téléchargement et extraction des données si elles n'existent pas\n",
    "- Séparation des données en jeux d'entraînement, de validation et de test (fonction auxilliaire contenue dans `prepare.py`)\n",
    "- Entraînement du tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517e4fd-d55d-431b-9267-49bc0e80d826",
   "metadata": {
    "id": "6517e4fd-d55d-431b-9267-49bc0e80d826"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "mkdir -p data\n",
    "\n",
    "if [ ! -f data/train.en-fr.fr ]; then\n",
    "    pushd data\n",
    "    wget -nc https://www.manythings.org/anki/fra-eng.zip\n",
    "    unzip -o fra-eng.zip\n",
    "    popd\n",
    "    python3 prepare.py data/fra.txt\n",
    "fi\n",
    "\n",
    "if [ ! -f data/bpecodes.en-fr ]; then\n",
    "    cat data/train.en-fr.{en,fr} | subword-nmt learn-bpe -o data/bpecodes.en-fr -s 8000 -v\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5mbjHVVaaVVd",
   "metadata": {
    "id": "5mbjHVVaaVVd"
   },
   "outputs": [],
   "source": [
    "# Définition de quelques variables utiles, et lecture du corpus parallèle\n",
    "data_dir = 'data/'\n",
    "source_lang = 'en'\n",
    "target_lang = 'fr'\n",
    "model_dir = 'models/{}-{}'.format(source_lang, target_lang)\n",
    "\n",
    "data_df = pd.read_csv(data_dir + 'fra.txt',\n",
    "                      encoding='UTF-8', sep='\\t', header=None,\n",
    "                      names=['eng', 'fra'], index_col=False)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eH1aKlz0aO_v",
   "metadata": {
    "id": "eH1aKlz0aO_v"
   },
   "source": [
    "#### Questions (tokenizer)\n",
    "\n",
    "Après avoir observé le résultat de l'exécution des deux cellules précédentes, répondre aux questions suivantes :\n",
    "\n",
    "1. Quelle est la taille de ce corpus, et que remarquez-vous à son sujet ?\n",
    "2. Comment fonctionne le tokenizer de type BPE ? Comment les différents sous-mots sont-ils construits ?\n",
    "3. Cherchez (Ctrl-F/Cmd-F) dans le résultat de l'exécution de la cellule précédent le mot \"danser\". À quelle fréquence apparait-îl dans le corpus, et de la fusion de quels tokens est-il issu ?\n",
    "4. Combien de tokens sont générés au total par l'algorithme de BPE ?  \n",
    "5. Les sous-mots sont calculés en même temps sur les mots français et anglais. À votre avis, pourquoi est-ce le cas (pourquoi ne pas le faire séparément) ?\n",
    "\n",
    "**Réponses :**\n",
    "1. Le corpus contient 232 736 paires de phrases, ou 465 472 phrases (la moitié en français et la moitié en anglais). Parmi les points notables, on peut voir qu'une même phrase anglaise a plusieurs traductions valides en français. On sait donc d'ores et déjà qu'il sera impossible d'atteindre un score de traduction maximal (puisque le modèle prédira toujours la même traduction pour chaque phrase anglaise)\n",
    "2. Le tokenizer BPE commence par créer un token pour chaque caractère possible (\"a\", \"b\", ..., \"z\", \"A\", ..., \"Z\", \".\", \"!\", ...). Ensuite, on cherche la paire de tokens avec la fréquence la plus haute dans le corpus (par exemple \"le\", constitué de \"l\" et de \"e\"). On transforme cette paire en un nouveau token, et on répète cette étape jusqu'à avoir le nombre total de tokens souhaité.\n",
    "3. La ligne `pair 3555: dan ser</w> -> danser</w> (frequency 172)` indique que le token \"danser\" est constitué de la fusion des tokens \"dan\" et \"ser\", eux-mêmes issues de la fusion de sous-tokens. Le mot \"danser\" apparaît 172 fois dans le corpus.\n",
    "4. Ici, on a demandé à l'algorithme de générer 8000 tokens.\n",
    "5. Le français et l'anglais sont issus de la même famille de langues (langues indo-européennes) et ont eu beaucoup d'influence mutuelle au cours de l'histoire. Il y a donc des similarités lexicales entre ces deux langues. Avoir les mêmes sous-mots pour les deux langues permet donc d'économiser de la mémoire, et peuvent aider le modèle à apprendre des caractéristiques communes à certains sous-mots entre les deux langues; par exemple, le suffixe -tion remplit globalement la même fonction en français et en anglais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0jUAa1AacVAn",
   "metadata": {
    "id": "0jUAa1AacVAn"
   },
   "source": [
    "## Chargement et prétraitement des données\n",
    "\n",
    "Le chargement et prétraitement des données consiste ici en 5 étapes :\n",
    "1. Charger le tokenizer BPE en mémoire,\n",
    "2. Charger les jeux d'entraînement, de validation et de test du corpus parallèle français-anglais. La fonction `load_data` va charger le corpus, puis le tokenizer en utilisant le fonction `preprocess`.\n",
    "3. Créer ou charger le dictionnaire associant les tokens à leur identifiants uniques (IDs) (fonction `nmt_dataset.load_or_create_dictionary`)\n",
    "4. Numériser les données en transformant chaque séquence source et cible en une séquences d'identifiants, et en les triant par taille (fonction `nmt_dataset.binarize`)\n",
    "5. Créer les lots d'entraînement (batches) (classe `nmt_dataset.BatchIterator`) en groupant ensemble les séquences d'une taille similaire. Les séquences sont ensuite complétées de zéros (*padding*) afin qu'elles aient toutes la même longueur. Toutes les séquences sont contenues dans des matrices numpy, qui seront utilisées pour entrainer les modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf175f-0545-4e57-8cda-9c4e90122df3",
   "metadata": {
    "id": "26bf175f-0545-4e57-8cda-9c4e90122df3"
   },
   "outputs": [],
   "source": [
    "# Reproducibilité des résultats\n",
    "def reset_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xwgcs5cOdXTu",
   "metadata": {
    "id": "xwgcs5cOdXTu"
   },
   "source": [
    "#### 1. Chargement du tokenizer (bilingue français-anglais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h3LzrxetdVWH",
   "metadata": {
    "id": "h3LzrxetdVWH"
   },
   "outputs": [],
   "source": [
    "bpe_path = os.path.join(data_dir, 'bpecodes.en-fr')\n",
    "\n",
    "with open(bpe_path) as bpe_codes:\n",
    "    bpe_model = BPE(bpe_codes)\n",
    "\n",
    "# Fonction qui tokenize un document\n",
    "def preprocess(line, is_source=True, source_lang=None, target_lang=None):\n",
    "    return bpe_model.segment(line.lower())\n",
    "\n",
    "# Fonction qui détokenize un document\n",
    "def postprocess(line):\n",
    "    return line.replace('@@ ', '')\n",
    "\n",
    "def load_data(source_lang, target_lang, split='train', max_size=None):\n",
    "    path = os.path.join(data_dir, '{}.{}-{}'.format(split, *sorted([source_lang, target_lang])))\n",
    "    return nmt_dataset.load_dataset(path, source_lang, target_lang, preprocess=preprocess, max_size=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47m3mJ7md4SC",
   "metadata": {
    "id": "47m3mJ7md4SC"
   },
   "source": [
    "#### 2. Chargement et prétraitement du corpus parallèle\n",
    "Comme évoqué plus haut, les données de notre corpus sont parallèles.\n",
    "Le code ci-dessous charge les données dans les partitions `train`, `dev` et `test` et affiche un exemple de données source et cible avec le resultat de la tokenization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7qGADilZdWDV",
   "metadata": {
    "id": "7qGADilZdWDV"
   },
   "outputs": [],
   "source": [
    "train_data = load_data(source_lang, target_lang, 'train', max_size=None)\n",
    "valid_data = load_data(source_lang, target_lang, 'valid')\n",
    "test_data = load_data(source_lang, target_lang, 'test')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PVsTtlf2eTz7",
   "metadata": {
    "id": "PVsTtlf2eTz7"
   },
   "source": [
    "##### Question (texte tokenisé)\n",
    "1. À votre avis, à quoi sert le symbole `@@` dans les textes tokenizés ?\n",
    "\n",
    "**Réponse :** Ce symbole représente la limite entre plusieurs tokens contenus dans un même mot. Pour reconstituer le mot, il faut donc coller tous les tokens puis supprimer ce symbole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A6z23yheeZJF",
   "metadata": {
    "id": "A6z23yheeZJF"
   },
   "source": [
    "#### 3. Création/Chargement du dictionnaire\n",
    "\n",
    "La prochaine étape est de determiner le vocabulaire utilisé dans les données. Ce vocabulaire est stocké dans deux listes (une pour chaque langue) appelées dictionnaires. Les dictionnaires sont triés par ordre de fréquence des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-T-SDyfId9qV",
   "metadata": {
    "id": "-T-SDyfId9qV"
   },
   "outputs": [],
   "source": [
    "source_dict_path = os.path.join(model_dir, 'dict.{}.txt'.format(source_lang))\n",
    "target_dict_path = os.path.join(model_dir, 'dict.{}.txt'.format(target_lang))\n",
    "\n",
    "source_dict = nmt_dataset.load_or_create_dictionary(\n",
    "    source_dict_path,\n",
    "    train_data['source_tokenized'],\n",
    "    minimum_count=10,\n",
    "    reset=False\n",
    ")\n",
    "\n",
    "target_dict = nmt_dataset.load_or_create_dictionary(\n",
    "    target_dict_path,\n",
    "    train_data['target_tokenized'],\n",
    "    minimum_count=10,\n",
    "    reset=False\n",
    ")\n",
    "print(source_dict.words[:100])\n",
    "\n",
    "print(target_dict.words[:100])\n",
    "print('Source vocab size:', len(source_dict))\n",
    "print('Target vocab size:', len(target_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TyRTXCkyfOYN",
   "metadata": {
    "id": "TyRTXCkyfOYN"
   },
   "source": [
    "##### Question (dictionnaire)\n",
    "1. Pourquoi un système de traduction automatique a-t-il besoin d'un dictionnaire ?\n",
    "2. Pourquoi utilise-t-on un vocabulaire acquis sur les données d'apprentissage et non pas un vocabulaire standard, qui serait plus universel ?\n",
    "3. À votre avis, que représente les quatre premiers éléments du vocabulaire ?\n",
    "4. Quelle est la taille du vocabulaire pour chaque langue ?\n",
    "5. En considerant le nombre de tokens connus par le tokenizer (section précédente) et la taille du vocabulaire, peut-on affirmer que les deux vocabulaires on des sous-mots en commun ?\n",
    "\n",
    "**Réponses :**\n",
    "1. Le dictionnaire permet tout simplement de convertir des mots (que l'on ne sait pas traiter de manière numérique) en indentifiants numériques (qui peuvent être traités). Il ne faut surtout pas répondre que le dictionnaire permet au système de connaître la traduction individuelle des mots, car ce n'est pas du tout le cas !\n",
    "2. La raison principale est que d'apprendre un vocabulaire acquis sur les données d'apprentissage permet au modèle d'être adapté à ces données. En effet, différents mots ou expressions peuvent avoir des significations différentes selon le corpus. De plus, certains mots peuvent potentiellement n'apparaître que dans les données d'entraînement et pas dans le corpus universel.\n",
    "3. `<sos>` (*start of sequence*) et `<eos>` (*end-of-sequence*) indiquent respectivement le début et la fin du document à traduire. `<unk>` (*unknown*) indique un mot ou token inconnu du dictionnaire (non présent dans les données d'entraînement). `<pad>` (*padding*) est un token de \"bourrage\", qui permet de remplir les séquences afin qu'elles aient toutes la même longueur.\n",
    "4. On a 4 193 tokens pour l'anglais et 5 218 pour le français\n",
    "5. 4 193 + 5 218 > 8 000. On a donc forcément des tokens en communs (plus exactement 1 411 tokens, soit 4 193 + 5 218 - 8 000) entre les deux langues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FZ4K04RNfoL2",
   "metadata": {
    "id": "FZ4K04RNfoL2"
   },
   "source": [
    "#### 4. Utilisation du dictionnaire pour associer les tokens à leurs indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bSFvPiJfEiK",
   "metadata": {
    "id": "2bSFvPiJfEiK"
   },
   "outputs": [],
   "source": [
    "nmt_dataset.binarize(train_data, source_dict, target_dict, sort=True)\n",
    "nmt_dataset.binarize(valid_data, source_dict, target_dict, sort=False)\n",
    "nmt_dataset.binarize(test_data, source_dict, target_dict, sort=False)\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4LS-ms-6f3Gf",
   "metadata": {
    "id": "4LS-ms-6f3Gf"
   },
   "source": [
    "#### Statistiques du corpus d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_LON_6q2fsd3",
   "metadata": {
    "id": "_LON_6q2fsd3"
   },
   "outputs": [],
   "source": [
    "print('train_size={}, valid_size={}, test_size={}, min_len={}, max_len={}'.format(\n",
    "    len(train_data),\n",
    "    len(valid_data),\n",
    "    len(test_data),\n",
    "    train_data['source_len'].min(),\n",
    "    train_data['source_len'].max(),\n",
    "))\n",
    "\n",
    "print('Distribution des longueurs de documents source (jeu d\\'entraînement) :')\n",
    "train_data['source_len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C5DEUeUjgNAn",
   "metadata": {
    "id": "C5DEUeUjgNAn"
   },
   "source": [
    "##### Question (statistiques du corpus)\n",
    "1. Combien de paires de phrases contient chacun des trois jeux de données (entraînement, validation et test) ?\n",
    "2. Dans le jeu d'entraînement, quelle est la longueur en tokens du document source le plus court et de celui le plus long ? Quelle est la taille mediane en tokens des documents source ?\n",
    "\n",
    "**Réponses :**\n",
    "1. On a 228 736 paires d'entraînement, 2 000 paires de validation et 2 000 paires de test, soit un total de 232 726 paires comme vu précédemment\n",
    "2. Dans le jeu d'entraînement, le document le plus court contient 3 tokens (dont `<sos>` et `<eos>`, donc un seul vrai token), le plus long en contient 85, et la longueur médiane est de 9 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Aobv9y-jg2Ha",
   "metadata": {
    "id": "Aobv9y-jg2Ha"
   },
   "source": [
    "#### 5. Construction des lots (batches)\n",
    "Nous regroupons ici les différents documents en lots (*batches*), ce qui permettra d'entraîner notre modèle sur plusieurs documents en parallèle et donc plus rapidement.\n",
    "Les lots sont automatiquement mélangés (shuffled) avant chaque itération d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QkuTI88Af6aH",
   "metadata": {
    "id": "QkuTI88Af6aH"
   },
   "outputs": [],
   "source": [
    "max_len = 30       # Maximum 30 tokens par document (les documents plus long seront tronqués)\n",
    "batch_size = 512   # Maximum 512 tokens par lot\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "train_iterator = nmt_dataset.BatchIterator(train_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=True)\n",
    "valid_iterator = nmt_dataset.BatchIterator(valid_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=False)\n",
    "test_iterator = nmt_dataset.BatchIterator(test_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=False)\n",
    "\n",
    "# Affiche le premier lot du jeu d'entraînement\n",
    "print(next(iter(train_iterator)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZKOIsVXchVD8",
   "metadata": {
    "id": "ZKOIsVXchVD8"
   },
   "source": [
    "Le modèle séquence vers séquence (seq2seq)\n",
    "=================\n",
    "\n",
    "Une approche naïve pour traduire les phrases consisterait à traduire chaque mot séparément, puis à recoller les traductions des mots individuels bout à bout. Cependant, cela ne marche pas. Considérons la phrase \"I am not the black cat\" → \"Je ne suis pas le chat noir\". La plupart des mots de la phrase d'entrée ont une traduction directe dans la phrase de sortie, mais sont dans un ordre légèrement différents, par exemple \"chat noir\" et \"black cat\". De plus, du fait de la construction \"ne/pas\", il y a un mot de moins dans la phrase d'entrée.\n",
    "\n",
    "Pour remédier à ce problème, un modèle seq2seq se compose généralement de deux parties, appelées *encodeur* et *décodeur*. L'encodeur lit une séquence d'entrée et produit un vecteur unique (encodage), qui correspond à une abstraction de la phrase d'entrée. Le décodeur lit ce vecteur pour produire une séquence de sortie, qui peut donc être de longueur et de forme différente de celle d'entrée.\n",
    "Le modèle seq2seq est indépendant de la longueur et de l'ordre des séquences, ce qui le rend idéal pour la traduction entre deux langues.\n",
    "\n",
    "![seq2seq](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "*Source de l'image: pytorch.org*\n",
    "\n",
    "L'encodeur\n",
    "-----------\n",
    "\n",
    "L'encodeur est la partie qui prend une phrase et nous en donne une représentation (vecteur). Dans la version la plus simple, l'encodeur d'un réseau seq2seq peut être un RNN, qui produit une valeur pour chaque mot de la phrase d'entrée. Plus précisément, pour chaque mot d'entrée, l'encodeur produit un vecteur et un état caché, et utilise l'état caché pour prédire le mot d'entrée suivant.\n",
    "\n",
    "Le décodeur\n",
    "-----------\n",
    "\n",
    "Le décodeur est un autre réseau de neurones, qui prend le(s) vecteur(s) de sortie de l'encodeur et produit une séquence de mots (traduction) à partir de celle-ci.\n",
    "\n",
    "Dans le décodeur seq2seq le plus simple, nous utilisons uniquement la dernière sortie de l'encodeur. Cette dernière sortie est parfois appelée le *vecteur de contexte*, car elle encode le contexte de la séquence entière. Ce vecteur de contexte peut être utilisé comme état caché initial pour un décodeur, qui peut être par exemple de type RNN.\n",
    "\n",
    "À chaque étape du décodage, le décodeur reçoit un token d'entrée et un état caché. Le token d'entrée initial est le token de début de chaîne et le premier état caché est le vecteur de contexte (le dernier état caché de l'encodeur).\n",
    "\n",
    "Le décodeur\n",
    "--------------------\n",
    "\n",
    "Le décodeur est un autre réseau RNN qui prend le ou les vecteurs de sortie de l'encodeur et produit une séquence de mots pour générer la traduction.\n",
    "\n",
    "Dans le décodeur seq2seq le plus simple, nous utilisons uniquement la dernière sortie de l'encodeur. Cette dernière sortie est parfois appelée le vecteur de contexte car elle code le contexte de la séquence entière. Ce vecteur de contexte peut être utilisé comme état caché initial pour un décodeur RNN.\n",
    "\n",
    "À chaque étape du décodage, le décodeur reçoit un token d'entrée et un état caché. Le token d'entrée initial est le token de début de chaîne et le premier état caché est le vecteur de contexte (le dernier état caché de l'encodeur).\n",
    "\n",
    "![Illustration encodeur-decodeur (si elle ne se charge pas, ouvrez l'image seq2seq.png)](seq2seq.png)\n",
    "\n",
    "## Encodeur et décodeur RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "onCdzq13mbpx",
   "metadata": {
    "id": "onCdzq13mbpx"
   },
   "outputs": [],
   "source": [
    "rnn_encoder = nnet_models.RNN_Encoder(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(rnn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MdKTYZaxmgeH",
   "metadata": {
    "id": "MdKTYZaxmgeH"
   },
   "outputs": [],
   "source": [
    "rnn_decoder = nnet_models.RNN_Decoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(rnn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89MkMCu5mi5O",
   "metadata": {
    "id": "89MkMCu5mi5O"
   },
   "outputs": [],
   "source": [
    "rnn_model = nnet_models.EncoderDecoder(\n",
    "    rnn_encoder,\n",
    "    rnn_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ijDpD75DhWMT",
   "metadata": {
    "id": "ijDpD75DhWMT"
   },
   "source": [
    "#### Questions (paramètres du modèle)\n",
    "\n",
    "À quoi correspondent les paramètres suivants ?\n",
    "1. `input_size=len(source_dict)`\n",
    "2. `output_size=len(target_dict)`\n",
    "3. `hidden_size=512`\n",
    "4. `num_layers=1`\n",
    "5. `dropout=0.2`\n",
    "6. `lr=0.001`\n",
    "\n",
    "**Réponses :**\n",
    "1. L'encodeur prend une entrée de type séquence de tokens. Le nombre d'identifiants possibles pour chaque token correspond à la taille du dictionnaire source (taille d'entrée).\n",
    "2. Idem pour la sortie du décodeur, avec le dictionnaire cible cette fois.\n",
    "3. La représentation d'un document (en sortie de l'encodeur/entrée du décodeur) sera un vecteur de taille 512.\n",
    "4. On a une seule couche cachée dans l'encodeur et le décodeur.\n",
    "5. 20% des neurones seront aléatoirement ignorés pendant chaque itération de l'entraînement afin de régulariser le réseau de neurones.\n",
    "6. Le taux d'apprentissage est de 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7VSzx6uriOGI",
   "metadata": {
    "id": "7VSzx6uriOGI"
   },
   "source": [
    "# Apprentissage du modèle de traduction\n",
    "\n",
    "## Programmation de l'apprentissage\n",
    "\n",
    "Comme dans les TP précédents, l'apprentissage s'effectue durant un certain nombre d'itérations. Pour verifier si l'apprentissage évolue positivement, nous utilisons deux mesures pour évaluer le modèle à chaque itération :\n",
    "- La fonction de coût du modèle\n",
    "- La mesure BLEU, qui est une métrique couramment utilisée en traduction. Cette métrique compare la sortie du modèle à une traduction de référence, et lui attribue un score.\n",
    "\n",
    "Le code ci-dessous sert à effectuer l'apprentissage du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VkAnBLjQhLUE",
   "metadata": {
    "id": "VkAnBLjQhLUE"
   },
   "outputs": [],
   "source": [
    "def save_model(model, checkpoint_path):\n",
    "    # Fonction servant à enregistrer le réseau de neurones\n",
    "    dirname = os.path.dirname(checkpoint_path)\n",
    "    if dirname:\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "    torch.save(model, checkpoint_path)\n",
    "\n",
    "def train_model(\n",
    "        train_iterator,\n",
    "        valid_iterators,\n",
    "        model,\n",
    "        checkpoint_path,\n",
    "        epochs=10,\n",
    "        validation_frequency=1\n",
    "    ):\n",
    "\n",
    "    reset_seed()  # Reproducibilité\n",
    "    epochsX = list(range(1, epochs+1))\n",
    "    epoch_lossY = []\n",
    "    epoch_loss_validation = []\n",
    "\n",
    "    best_bleu = -1\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "        running_loss_dev = 0\n",
    "\n",
    "        print('Itération : [{}/{}]'.format(epoch, epochs))\n",
    "\n",
    "        # Enumère tous les lots d'entraînement\n",
    "        for i, batch in tqdm(enumerate(train_iterator), total=len(train_iterator)):\n",
    "            t = time.time()\n",
    "            running_loss += model.train_step(batch)\n",
    "\n",
    "        # Calcule la fonction de coût moyenne sur le jeu d'entraînement pour cette itération\n",
    "        epoch_loss = running_loss / len(train_iterator)\n",
    "        epoch_lossY.append(epoch_loss)\n",
    "\n",
    "        print(\"Coût (train)={:.3f}, durée={:.2f}\".format(epoch_loss, time.time() - start))\n",
    "\n",
    "        # Enumère tous les lots de validation\n",
    "        for i, batch in tqdm(enumerate(valid_iterators[0]), total=len(valid_iterators[0])):\n",
    "            t_dev = time.time()\n",
    "            running_loss_dev += model.dev_step(batch)\n",
    "\n",
    "        # Calcule la fonction de coût moyenne sur le jeu de validation pour cette itération\n",
    "        epoch_loss_dev = running_loss_dev / len(valid_iterators[0])\n",
    "        epoch_loss_validation.append(epoch_loss_dev)\n",
    "\n",
    "        print(\"Coût (valid)={:.3f}, durée={:.2f}\".format(epoch_loss_dev, time.time() - start))\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Évalue et enregistre le modèle\n",
    "        if epoch % validation_frequency == 0:\n",
    "            bleu_scores = []\n",
    "\n",
    "            # Calcule le score BLEU sur tous les lots de validation\n",
    "            for valid_iterator in valid_iterators:\n",
    "                src, tgt = valid_iterator.source_lang, valid_iterator.target_lang\n",
    "                translation_output = model.translate(valid_iterator, postprocess)\n",
    "                bleu_score = translation_output.score\n",
    "                output = translation_output.output\n",
    "\n",
    "                with open(os.path.join(model_dir, 'valid.{}-{}.{}.out'.format(src, tgt, epoch)), 'w') as f:\n",
    "                    f.writelines(line + '\\n' for line in output)\n",
    "\n",
    "                print('{}-{}: BLEU={}'.format(src, tgt, bleu_score))\n",
    "                sys.stdout.flush()\n",
    "                bleu_scores.append(bleu_score)\n",
    "\n",
    "            # Calcule le score BLEU moyen obtenu\n",
    "            bleu_score = round(sum(bleu_scores) / len(bleu_scores), 2)\n",
    "            if len(bleu_scores) > 1:\n",
    "                print('BLEU={}'.format(bleu_score))\n",
    "\n",
    "            # Met à jour le taux d'apprentissage du réseau de neurones en fonction du score BLEU.\n",
    "            # Ici, le taux d'apprentissage est divisé par 10 si le score BLEU n'augmente pas.\n",
    "            model.scheduler_step(bleu_score)\n",
    "\n",
    "            # Si le réseau de neurones obtient un score BLEU plus élevé qu'à toutes les itérations précédentes, on l'enregistre\n",
    "            if bleu_score > best_bleu:\n",
    "                best_bleu = bleu_score\n",
    "                save_model(model, checkpoint_path)\n",
    "\n",
    "        print('=' * 50)\n",
    "\n",
    "    print(\"Apprentissage terminé. Meilleure score BLEU : {}\".format(best_bleu))\n",
    "\n",
    "    plt.plot(epochsX, epoch_lossY, '--x', label='Training Loss')\n",
    "    plt.plot(epochsX, epoch_loss_validation, '--x', label='Validation Loss')\n",
    "    plt.ylim(0, np.max(epoch_lossY) + 1)\n",
    "    plt.xticks(range(1,epochs+1))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Epoch Loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2WiWTLJ0oXkD",
   "metadata": {
    "id": "2WiWTLJ0oXkD"
   },
   "source": [
    "Note : Il faut savoir que les termes _dev_ et _validation_ désignent le même jeu de données.\n",
    "\n",
    "#### Question (score BLEU)\n",
    "1. Entre quelle valeur et quelle valeur peut être compris un score BLEU ? Vaut-il mieux que la valeur obtenue soit élevée ou faible ? Pourquoi ?\n",
    "\n",
    "**Réponse :** Un score BLEU peut aller de 0 à 1, ou de 0 à 100 en mode pourcentage. Une valeur élevée correspond à une meilleure performance, car cela signifie que la phrase évaluée est plus proche de la phrase de référence. Cependant, il s'agit d'une comparaison purement syntaxique; un modèle qui génère des traductions parfaites mais formulées différemment de celles de référence obtiendrait donc un score plus bas que ce à quoi on peut s'attendre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LO42QpnIom2p",
   "metadata": {
    "id": "LO42QpnIom2p"
   },
   "source": [
    "## Execution de l'apprentissage du modèle de traduction\n",
    "\n",
    "**Remarque :** l'apprentissage peut durer longtemps (plusieurs minutes). Pour éviter cela, vous pouvez régler `epoch` (le nombre d'itérations) sur une petite valeur, par exemple 2, et relancer cette cellule plusieurs fois pour continuer à entraîner votre modèle (`train_model` ne réinitialise pas le modèle). Si l'apprentissage dure plus de 10 minutes, vérifiez que vous avez bien sélectionne un GPU comme demandé au tout début du TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MSJrYjIPhUGT",
   "metadata": {
    "id": "MSJrYjIPhUGT"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(model_dir, 'rnn.pt')\n",
    "\n",
    "train_model(train_iterator, [valid_iterator], rnn_model,\n",
    "            epochs=10,\n",
    "            checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cewKdyP_rjbe",
   "metadata": {
    "id": "cewKdyP_rjbe"
   },
   "source": [
    "#### Question (entraînement)\n",
    "Observez la figure présentant la fonction de coût (*loss*) à chaque itération (au moins 5 itérations au total), puis répondez aux questions suivantes :\n",
    "1. Comment le coût évolue-t-il avec le nombre d'époques ?\n",
    "2. Est-ce que le modèle pourrait poursuivre son entrainement, ou au contraire, voit-on apparaitre un sur-apprentissage ?\n",
    "\n",
    "**Réponses :**\n",
    "1. Les deux coûts ont tendance à diminuer au fur et à mesure que les époques sont effectuées, mais le coût de validation stagne rapidement.\n",
    "2. Le coût de validation commence à stagner après 6 époques, il est donc possible que l'on voie le début d'un sur-apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rsqPyjuHsUD-",
   "metadata": {
    "id": "rsqPyjuHsUD-"
   },
   "source": [
    "## Evaluation du modèle sur le corpus de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1xS89wrfYW",
   "metadata": {
    "id": "3a1xS89wrfYW"
   },
   "outputs": [],
   "source": [
    "print('BLEU:', rnn_model.translate(test_iterator, postprocess).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_Np-wvkvsZNs",
   "metadata": {
    "id": "_Np-wvkvsZNs"
   },
   "source": [
    "## Test interactif du modèle\n",
    "\n",
    "Vous pouvez tester votre modèle de manière interactive soumettant des phrases à traduire de votre choix. Il n'est pas nécessaire de lire ou comprendre le code ci-dessous, il suffit de modifier les phrases de test dans le bloc de code situé après les définitions de fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RnpMt2SXo8GW",
   "metadata": {
    "id": "RnpMt2SXo8GW"
   },
   "outputs": [],
   "source": [
    "def get_binned_bleu_scores(model, valid_iterator):\n",
    "    lengths = np.arange(4, 20, 3)\n",
    "    bleu_scores = np.zeros(len(lengths))\n",
    "\n",
    "    for i in tqdm(range(1, len(lengths)), total=len(lengths) - 1):\n",
    "        min_len = lengths[i - 1]\n",
    "        max_len = lengths[i]\n",
    "\n",
    "        tmp_data = valid_data[(valid_iterator.data['source_len'] > min_len) & (valid_iterator.data['source_len'] <= max_len)]\n",
    "        tmp_iterator = nmt_dataset.BatchIterator(tmp_data, source_lang, target_lang, batch_size, max_len=max_len)\n",
    "\n",
    "        bleu_scores[i] = model.translate(tmp_iterator, postprocess).score\n",
    "\n",
    "    lengths = lengths[1:]\n",
    "    bleu_scores = bleu_scores[1:]\n",
    "\n",
    "    plt.plot(lengths, bleu_scores, 'x-')\n",
    "    plt.ylim(0, np.max(bleu_scores) + 1)\n",
    "    plt.xlabel('Source length')\n",
    "    plt.ylabel('BLEU score')\n",
    "\n",
    "    return lengths, bleu_scores\n",
    "\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions, cmap='bone', aspect='auto')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + [nmt_dataset.EOS_TOKEN], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ') + [nmt_dataset.EOS_TOKEN])\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def encode_as_batch(sentence, dictionary, source_lang, target_lang):\n",
    "    sentence = sentence + ' ' + nmt_dataset.EOS_TOKEN\n",
    "    tensor = dictionary.txt2vec(sentence).unsqueeze(0)\n",
    "\n",
    "    return {\n",
    "        'source': tensor,\n",
    "        'source_len': torch.from_numpy(np.array([tensor.shape[-1]])),\n",
    "        'source_lang': source_lang,\n",
    "        'target_lang': target_lang\n",
    "    }\n",
    "\n",
    "\n",
    "def get_translation(model, sentence, dictionary, source_lang, target_lang, return_output=False):\n",
    "    print('Source:', sentence)\n",
    "    sentence_tok = preprocess(sentence, is_source=True, source_lang=source_lang, target_lang=target_lang)\n",
    "    print('Tokenized source:', sentence_tok)\n",
    "    batch = encode_as_batch(sentence_tok, dictionary, source_lang, target_lang)\n",
    "    prediction, attn_matrix, enc_self_attn = model.eval_step(batch)\n",
    "    prediction = prediction[0]\n",
    "    prediction_detok = postprocess(prediction)\n",
    "    print('Prediction:', prediction)\n",
    "    print('Detokenized prediction:', prediction_detok)\n",
    "\n",
    "    print('Google Translate ({}->{}): {}'.format(\n",
    "        source_lang,\n",
    "        target_lang,\n",
    "        translator.translate(sentence, src=source_lang, dest=target_lang).text\n",
    "    ))\n",
    "    print('Google Translate on prediction ({}->{}): {}'.format(\n",
    "        target_lang,\n",
    "        source_lang,\n",
    "        translator.translate(prediction_detok, src=target_lang, dest=source_lang).text\n",
    "    ))\n",
    "\n",
    "    results = {\n",
    "        'source': sentence,\n",
    "        'source_tokens': sentence_tok.split(' ') + ['<eos>'],\n",
    "        'prediction_detok': prediction_detok,\n",
    "        'prediction_tokens': prediction.split(' '),\n",
    "    }\n",
    "\n",
    "    if attn_matrix is not None:\n",
    "        attn_matrix = attn_matrix[0].detach().cpu().numpy()\n",
    "        results['attention_matrix'] = attn_matrix\n",
    "        show_attention(sentence_tok, prediction, attn_matrix)\n",
    "\n",
    "    if enc_self_attn is not None:\n",
    "        results['encoder_self_attention_list'] = enc_self_attn\n",
    "\n",
    "    if return_output:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IHogqMbQsta6",
   "metadata": {
    "id": "IHogqMbQsta6"
   },
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'hello how are you ?', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4wkLdSmEs0px",
   "metadata": {
    "id": "4wkLdSmEs0px"
   },
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'are hello ? how you', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8nEcrJrks1aI",
   "metadata": {
    "id": "8nEcrJrks1aI"
   },
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'she \\'s five years older than me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fK97s-lvs2m0",
   "metadata": {
    "id": "fK97s-lvs2m0"
   },
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'i know that the last thing you want to do is help me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BdCmF5j9s7xg",
   "metadata": {
    "id": "BdCmF5j9s7xg"
   },
   "source": [
    "## Tracé du score BLEU en fonction de la longueur de la séquence source\n",
    "\n",
    "Comme vu en cours, les performances des modèles RNNs chutent lorsque la longueur de l'entrée augmente.\n",
    "Ceci est dû à trois facteurs principaux :\n",
    "- Le décodeur RNN se base uniquement sur le dernier état caché de l'encodeur. Cela signifie que nous devons encoder la phrase complète dans un vecteur unique de taille fixe. Plus la phrase est longue, plus cela est difficile.\n",
    "- Les RNN encodeurs-décodeurs sont difficiles à entraîner (car le signal doit être rétro-propagé à travers toute la séquence d'états).\n",
    "- L'ensemble d'apprentissage que nous avons utilisé est principalement composé de phrases très courtes (95 % des phrases sources comportent 15 éléments ou moins).\n",
    "\n",
    "Vous pouvez visualiser ce phénomène sur le graphique suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C2aNzRKEs4R7",
   "metadata": {
    "id": "C2aNzRKEs4R7"
   },
   "outputs": [],
   "source": [
    "rnn_lengths, rnn_bleu_scores = get_binned_bleu_scores(rnn_model, valid_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GGzyw_X7thYN",
   "metadata": {
    "id": "GGzyw_X7thYN"
   },
   "source": [
    "Le modèle séquence-vers-séquence (seq2seq) avec attention\n",
    "=================\n",
    "\n",
    "\n",
    "Le vecteur passé au décodeur doit représenter l'ensemble des informations de l'entrée, ce qui est trop peu, notamment pour les phrases longues. Le mécanisme d'attention est un moyen de recalculer un lien avec les éléments importants de l'entrée à chaque pas de décodage. Ainsi, dans la figure ci-dessous, le terme \"Je\" dépend plus fortement de termes \"I\" et \"am\" que de \"a\" et \"student\". Ceci permet au modèle d'être plus flexible et de prendre en compte des dépendances qui peuvent être assez longues.\n",
    "\n",
    "\n",
    "\n",
    "![seq2seq_avec_attention](https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg)\n",
    "\n",
    "\n",
    "\n",
    "*Source de l'image: www.tensorflow.org*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Les architectures avec mécanismes d'attention connaissent beaucoup de succès dans de nombreuses applications comme la traduction, la reconnaissance de la parole, la description d'image, etc. ([cet article de blog constitue une excellente introduction en la matière](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)).\n",
    "\n",
    "Ici, nous n'avons pas besoin de modifier l'architecture de l'encodeur existant. En revanche, le décodeur fonctionne différemment : en ajoutant un mécanisme d'attention, à chaque étape du décodage, le décodeur reçoit un token d'entrée, l'état caché précédent et un vecteur qui est calculé directement à partir de la séquence d'entrée. Ainsi, à chaque étape de décodage, le décodeur peut donner plus ou moins d'**attention** à des tokens de la source, ce qui permet de prendre en compte des dépendances plus longues que dans la version sans attention.\n",
    "\n",
    "## Encodeur et décodeur RNN avec attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kuN_4E41tNMd",
   "metadata": {
    "id": "kuN_4E41tNMd"
   },
   "outputs": [],
   "source": [
    "rnn_attn_encoder = nnet_models.RNN_Encoder(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(rnn_attn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "So7G_iDuuPYG",
   "metadata": {
    "id": "So7G_iDuuPYG"
   },
   "outputs": [],
   "source": [
    "rnn_attn_decoder = nnet_models.AttentionDecoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(rnn_attn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1Jsgq3buuP7V",
   "metadata": {
    "id": "1Jsgq3buuP7V"
   },
   "outputs": [],
   "source": [
    "rnn_attn_model = nnet_models.EncoderDecoder(\n",
    "    rnn_attn_encoder,\n",
    "    rnn_attn_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8g0z2Nn5urwG",
   "metadata": {
    "id": "8g0z2Nn5urwG"
   },
   "source": [
    "## Apprentissage du modèle\n",
    "\n",
    "**Remarque :** l'entrainement est encore plus long que précédemment (3 minutes par itération). À nouveau, il est recommandé de procéder en n'entraînant que pendant une ou deux itérations à la fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LBnPdDkLuQcC",
   "metadata": {
    "id": "LBnPdDkLuQcC"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(model_dir, 'rnn_attn.pt')\n",
    "\n",
    "train_model(train_iterator, [valid_iterator], rnn_attn_model,\n",
    "            epochs=1,\n",
    "            checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o75Ccx0Du_3T",
   "metadata": {
    "id": "o75Ccx0Du_3T"
   },
   "source": [
    "## Évaluation du modèle sur le corpus de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ymU45phVuo8Z",
   "metadata": {
    "id": "ymU45phVuo8Z"
   },
   "outputs": [],
   "source": [
    "print('BLEU:', rnn_attn_model.translate(test_iterator, postprocess).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q7oHRe11vD9S",
   "metadata": {
    "id": "Q7oHRe11vD9S"
   },
   "source": [
    "#### Question (encodeur avec attention)\n",
    "\n",
    "*Il est possible de répondre à cette question après une seule itération d'apprentissage.*\n",
    "\n",
    "1. Après avoir ajouté le mécanisme d'attention, observez-vous un gain de performance ? À votre avis, pourquoi ?\n",
    "\n",
    "**Réponse :** Dépend de votre machine. En général, on peut observer un gain de performance assez faible. On peut justifier que les performances sont meilleures (car l'attention est plus efficace) ou pas (car le modèle est trop petit, on n'utilise pas assez de données d'entraînement, le score BLEU n'est pas adapté, l'attention a besoin de plus de temps pour être apprise...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urBVnzafwL9x",
   "metadata": {
    "id": "urBVnzafwL9x"
   },
   "source": [
    "## Test interactif du modèle et visualisation des matrices d'attention\n",
    "\n",
    "Vous pouvez tester votre modèle de manière interactive grâce au code ci-dessous, en soumettant des phrases à traduire de votre choix.\n",
    "\n",
    "Pour chacune des traductions ci-dessous, vérifiez si le mécanisme d'attention permet bien de focaliser la prédiction du prochain mot dans la langue cible sur les mots importants de la séquence d'entrée dans la langue source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HaZco3PZveOw",
   "metadata": {
    "id": "HaZco3PZveOw"
   },
   "outputs": [],
   "source": [
    "get_translation(rnn_attn_model, 'hello how are you ?', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j3GjsYsxwO0n",
   "metadata": {
    "id": "j3GjsYsxwO0n"
   },
   "outputs": [],
   "source": [
    "get_translation(rnn_attn_model, 'are hello ? how you', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L2prODqVwPfA",
   "metadata": {
    "id": "L2prODqVwPfA"
   },
   "outputs": [],
   "source": [
    "get_translation(rnn_attn_model, 'she \\'s five years older than me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hxQqyh0mwQA-",
   "metadata": {
    "id": "hxQqyh0mwQA-"
   },
   "outputs": [],
   "source": [
    "get_translation(rnn_attn_model, 'i know that the last thing you want to do is help me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PfrFzcJvx0Y4",
   "metadata": {
    "id": "PfrFzcJvx0Y4"
   },
   "source": [
    "# Bonus : Optimisation des paramètres\n",
    "\n",
    "Pour l'instant, le modèle obtient des scores moyens. Charge à vous de faire évoluer la configuration afin d'obtenir un modèle plus performant. Plusieurs paramètres peuvent être modifiés afin d'obtenir de meilleures traductions. Dans le premier TP, nous avons fait des expérimentations en changeant les paramètres `epoch` et `lr`. Dans ce TP, essayez de faire évoluer les paramètres décrits ci-après et observez la performance du modèle.\n",
    "\n",
    "Afin de bien comprendre l'importance de chaque paramètre dans la performance globale du système, veillez dans un premier temps à changer les paramètres un à un (et non tous en même temps) et notez les résultats. De la sorte, vous pourrez quantifier précisément le rôle de chaque paramètre.\n",
    "\n",
    "Vous pouvez également faire varier les paramètres suivants :\n",
    "\n",
    "- **Taille des lots** : Faites varier `batch_size` (ex. 512 -> 256 -> 128, n'allez pas au-delà de 512).\n",
    "- **Nombre de couches** : Augmentez `num_layers` (n'allez pas au-delà de 3 couches)\n",
    "- **Nombre de neurones** : Faites varier `hidden_size` (ex. 512 -> 256 -> 128, n'allez pas au-delà de 512)\n",
    "- **Dropout** : Modifiez la valeur de `dropout` (ex. 0.0 -> 0.1 -> 0.2)\n",
    "- **Taille de la partition dev** : Initialement `train_size=186206`, `valid_size=2000`, `test_size=2000`, `min_len=3`, `max_len=69`. Vous pouvez modifier le nombre des données de validation dans le fichier `prepare.py` et recharger le fichier.\n",
    "\n",
    "En tenant compte des paramètres optimaux de ces TPs, essayez de choisir la combinaison de paramètres la plus efficace possible pour atteindre le meilleur score BLEU. Vous pouvez changer les paramètres que vous venez de tester ainsi que les autres paramètres disponibles pour l'architecture du modèle, son entraînement et le décodage.\n",
    "\n",
    "\n",
    "### Question (optimisation)\n",
    "1. À la fin de ce TP, quel est le meilleur score BLEU que vous pouvez obtenir ?\n",
    "\n",
    "[ **répondre ici** ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qOF2jA08wQu5",
   "metadata": {
    "id": "qOF2jA08wQu5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
